Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2015-05-22T18:27:27+05:30

====== elastic solr ======
Created Friday 22 May 2015
http://ecmarchitect.com/archives/2015/07/27/4031

running in docker:
http://elk-docker.readthedocs.io/
increase vm as es needs it:
sysctl -w vm.max_map_count=262144

docker run -p 5601:5601 -p 9200:9200 -p 5044:5044 -p 9300:9300 -v /data/es-docker-data/:/var/lib/elasticsearch -it --name elk sebp/elk

docker run -p 5601:5601 -p 9200:9200 -p 5044:5044 -p 9300:9300 -v /data/es_docker_data/:/var/lib/elasticsearch -it --name elk my_elk

uses optimistic concurrency control, provide version number to each record, so that we don't miss anything
so we don;t need locking documents, which would have been difficult in distributed system

/index/type/id
PUT [[/college/student/1?version=3]] -d{  ... } if we change older version it will returen 409 conflict

POST [[/college/student/1/_update]] -d { ... } , 
GET -> change -> PUT
herewe can pass retry_on_conflict: meaning in distributed environment if someone else has changed before us, version# will change so try this many times

bulk indexing and searching is better than document by document

pagination /_search?size=5&from=5

/_search?q=name:deepak+sem:1
sem must be present, need % encoding also

[[/college/student/_mapping]] gives info of schema; datatype for each field
we can update mapping by just PUT new json object for mapping, but we can't change existing fields

process:
tokenization: for full text search, it gets separated words/terms from document, sorts them, create DS like how many times term appears in that document
term | doc1 | doc2 | doc3

term normalization is used while inserting data, stemming, words with same meaning etc....
term normalization is done at query time too

this process of tokenization and normalization is called **analysis**

so analyzer consist of character filter, tokenizer ,token filters

fields can be:
boolean, byte, short, integer, long, float, double, string, date, object and multi_field, ip, geo_point, geo_shape

for full text search, datatype  index is analysed, if want exact seaarch use not_analysed instead
index:no makes strign not searchabel

Filter:
exact matching, exists yes/no, fast ,cacheable
Query:
full text search, relevance scoring, heavier, not cacheable

apart from searching, we sometime require auto complete, but word prefixes are not yet indexed, so we use N-grams

its not good for start of word, so we use Edge N-gram token filter
but we need to apply these filters to name field mapping, but we also want to keep original field, so we use type:"multi_field"
then we need to reindex all documents

bool query:
must
must_not
should

gaussian curve with decreasing boost over geo query



https://www.youtube.com/watch?v=7FLXjgB0PQI

basic:
http://cdn.oreillystatic.com/en/assets/1/event/115/An%20Elasticsearch%20Crash%20Course%20Presentation.pdf

eg: snowball analyser
analyzer: tokenize text and outputs terms, => stemming and stopwords
ngrams: news => n, e, w, s, ne , ew, ws

there are various analysers: like path heirarchy analyser

scorign = relevance
choice of various scorign algorithms

query can fall into various categorise, we can compose queries using bool/dismax

if type is defined for index, all documents in it should have fields conforming to that type

books:
https://www.elastic.co/guide/en/elasticsearch/guide/master/intro.html
http://exploringelasticsearch.com/modeling_data.html#ch-modeling-data

in kopf
it will autodetect type of extra coloumn and allow insertion


for allowing 
[[/_cluster/settings]] transient

for search:
_search 
[[/nodename/collection/_search]] 

use 
GET: 
PUT: insett/update
DELETE: delete
http://localhost:9200/<index>/<type>/[<id>]

http://localhost:9200/movies/movie/_search

Basic free text search
query-string-query: use POST
{
	"query": {
		"query_string": {
			"query": "kill"
		}
	}
}

use  "fields": ["title"] to search in title field only

A filtered query is a query that has two properties, query and filter. When executed it filters the result of the query using the filter.

{
	"query": {
		"filtered": {
			"query": {
				"query_string": {
					"query": "drama"
				}
			},
			"filter": {
				//Filter to apply to the query
			}
		}
	}
}

match is used for matching words in document; eg "rock climbing" will search docs with rock and climbing individually according to relevance order sorting.
use match_phrase for exact word searching

Duplicating each shard to provide redundant copies of your data, to prevent data loss in case of hardware failure
Routing requests from any node in the cluster to the nodes that hold the data you’re interested in

One node in the cluster is elected to be the master node, which is in charge of managing cluster-wide changes like creating or deleting an index, or adding or removing a node from the cluster. The master node does not need to be involved in document-level changes or searches, which means that having just one master node will not become a bottleneck as traffic grows. 
Every node knows where each document lives and can forward our request directly to the nodes that hold the data we are interested in. Whichever node we talk to manages the process of gathering the response from the node or nodes holding the data and returning the final response to the client.

In reality, an index is just a logical namespace that points to one or more physical shards.

optimistic concurrency control says that you get document and send ?version={ retreived from doc} during updation. elastic won't allow if other transaction has changed version#, it will ask you to reget data

This explains why the number of primary shards can be set only when an index is created and never changed
shard = hash(routing) % number_of_primary_shards

Mapping
	How the data in each field is interpreted
Analysis
	How full text is processed to make it searchable
Query DSL
	The flexible, powerful query language used by Elasticsearch

The mapping defines how each field in the document is analyzed.
 Elasticsearch first analyzes the text, and then uses the results to build an inverted index. which is designed to allow very fast full-text searches

You can find only terms that exist in your index, so both the indexed text and the query string must be normalized into the same form.

we can use _analyze to try out various analyzers.
we we insert data elastic uses standard analyzer for full text serch, we can use mapping to change analysis.
each document in an index has a type. Every type has its own mapping, or schema definition. A mapping defines the fields within a type, the datatype for each field, and how the field should be handled by Elasticsearch. A mapping is also used to configure metadata associated with the type.

Query clauses are simple building blocks that can be combined with each other to create complex queries.
Leaf clause: match
Compound clause: bool, must, nust_not, should

When used in filtering context, the query is said to be a "non-scoring" or "filtering" query. When used in a querying context, the query becomes a "scoring" query.  it also determines how well does the document matches.

/_validate/query can be used to check if query is valid, ?explain gives reason for invalidation

if we have text field that we want to analyze for searching but keep it same for sorting. we can either use two fileds or use
	"fields": {
		"raw": { 
			"type":  "string",
			"index": "not_analyzed"
		}
	}

Character filters are used to “tidy up” a string before it is tokenized. eg: we want to strip html tags before analysing text

there is functionality of aliase, we can alias index to index_v1, then creating index_v2 after making changes, we can alias index to index_v2.

 a Lucene index is what we call a shard in Elasticsearch, while an index in Elasticsearch is a collection of shards.

field names can be used as regular expression; like *_title

Full-text search is a battle between recall—returning all the documents that are relevant—and precision—not returning irrelevant documents. 

default **similarity algorithm** used to calculate the relevance score for each term is TF/IDF

for fields first_name, last_name; eg: peter smith, smith williams puts smith williams higher in score due to low IDF of smith as first_name. we can use full_name for better results

 The cross_fields type takes a term-centric approach, quite different from the field-centric approach taken by best_fields and most_fields.

 a simple term query is about 10 times as fast as a phrase query, and about 20 times as fast as a proximity query (a phrase query with slop). And of course, this cost is paid at search time instead of at index time.

Sue ate the alligator: unigram indexing ["sue", "ate", "the", "alligator"]
bigram **shingles** ["sue ate", "ate the", "the alligator"] 

if some field is not analysed like post-code; prefix, wildcard, regexp queries can be used


term frequency: tf(t in d) = √frequency 
inverse document frequency: idf(t) = 1 + log ( numDocs / (docFreq + 1)) 
field length norm: norm(d) = 1 / √numTerms 

sometimes we don't care how many times say pool comes in description; we want it should just be present; we can use constant_score query for that


**Buckets**
Collections of documents that meet a criterion
**Metrics**
Statistics calculated on the documents in a bucket

SELECT COUNT(color)  FROM table GROUP BY color 
here group by color is bucket, count(color) is metric
Buckets are conceptually similar to grouping in SQL, while metrics are similar to COUNT(), SUM(), MAX(), and so forth.

While fields of type object (see Multilevel Objects) are useful for storing a single object, they are useless, from a search point of view, for storing an array of objects.




https://www.elastic.co/guide/en/elasticsearch/reference/1.4/glossary.html

query context generates score, how well this document matches query
filter context doesn't  change score of documents, we use it to filter out results of query

_score can be changed with the boost

extra ' caused js in kopf to crash, see entries in browser and delete entry from index

Use Elastica.io

 Java clients talk to the cluster over port 9300, using the native Elasticsearch transport protocol. The nodes in the cluster also communicate with each other over port 9300.

**how to decide index, types**
	https://www.elastic.co/blog/index-vs-type
		basically index is main thing for storing data; type is just a field used for filtering automatically. also if different types have same name they should have same data-types as mapping is done for index, not for each types' fields.

**updating a document**
https://www.elastic.co/guide/en/elasticsearch/guide/current/update-doc.html
https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html

for geo maps:
string: lat, lon same goes for google maps url
array: [ lon, lat ]

relation b/w index type and name provided in mapping:
while creating mappings for index
 "mappings": {
	  "organization-mapping" : {
	}
}

here organization-mapping should be same as type name used while inserting document.

put /index-name/organization-mapping/id
otherwise mapping rules won't apply

when should we create type in same index and in different index
	https://www.elastic.co/guide/en/elasticsearch/guide/current/mapping.html#_avoiding_type_gotchas

what happens if we run term query on analyzed field
	ES doesn't complain, ~~but it won't match term with field even though it is present in that field, it gives back null~~
	it will match in the field, eg: term was "king", and analyzed field was "the king is", then term query will match
	so term query can be applied on analyzed field and it will calculate score
	
if we run match query on not_analyzed field, it works if we provide exact query string; it works with term query obviously.

term query uses score, We use a constant_score to convert the term query into a filter.

It is important to understand that term and terms are contains operations, not equals. the best way to accomplish it involves indexing a secondary field. eg "tag_count" : 2 is added along with. this field can be used while searching.

 the match or query_string queries are high-level queries that understand the mapping of a field. if you use it against date/integer field it will use input query string as date/integer field, if used against not_analyzed field it will use input query string as single term; if used against field having analyzed mapping, it will pass query string through appropriate analyzer and do full text search

should field is used for scoring, we can apply boost to some term in query

__quest__
~~bool, term, filter~~
~~aggregation, shards vs nodes, opetators: bool not etc, should have ~~
~~score in elastic search~~
~~how mapping and routign works~~ 
~~field, mapping, routing, term, text~~
~~facets, boost~~
~~rewrite, boost, fuzziness~~
~~how to remember and which query should be used~~
~~how is json key value pair stored; what happens to key~~

internals of index sharded, how lucene indexing happens for inverted index
	
	~~it looks like based on hash of _id of doc we can find shardId; but how is inverted indexes stored then~~
~~how elastic match_phrase will happen if document is inverted indexed word wises~~

~~if we update/delete a document, is it still indexed and searchable~~
we can't change #primary shards; how to determine their numbers beforehand
how to abort long runing queries
what are segments
wouldn't we require to reindex even if we want to change stem_exclusion, stop-words; how is it handled dynamically changing requirements
when we should use nested objects or parent-child relationships
~~elastic search why term query have relevance?~~

__todo__
~~after normalizing tokens~~
read about how lucene stores data


********************************* solr *************************************
 adding documents to Solr, you need to specify the schema, represented in a file called schema.xml.

 the stored attribute: to tell Solr to store the original text in the index somewhere. there are fields which aren't searched, but need to be displayed in the search results. You accomplish that by setting the field attributes to stored=true and indexed=false.

https://cwiki.apache.org/confluence/display/solr/Getting+Started


bucket aggs, filter, analysis





