Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2018-06-18T17:24:38+05:30

====== kafka ======
Created Monday 18 June 2018

https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html

https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka

user: kafka
pass: times41!

installed on /opt/kafka
cd /opt/kafka/kafka_2.11-1.1.0/
run zookeper: ./bin/zookeeper-server-start.sh config/zookeeper.properties
./bin/kafka-server-start.sh config/server.properties

./bin/kafka-topics.sh --create   --zookeeper localhost:2181  --replication-factor 1 --partitions 13 --topic my-topic
./bin/kafka-topics.sh --list --zookeeper localhost:2181

./bin/kafka-console-producer.sh --broker-list localhost:9092   --topic my-topic

./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning



use kafka manager
cd  /opt/kafka-manager/target/universal/kafka-manager-1.3.3.17/
./bin/kafka-manager -Dkafka-manager.zkhosts="localhost:2181" -Dhttp.port=8181

use kafka kadmin
cd /opt/kafka-kadmin/kadmin/dist
java -jar shared-kafka-admin-micro-0.9.1.jar --spring.profiles.active=desired,Spring,profiles --server.port=9191

running zookeeper is required even fro single broker node cluster
kafka maintains order of message in same partition only, we can send message concerned message using partition key to same partition
kafka doesn't itself delete message, we can use retention policy/disk quota for auto removal OR
stop cluster and delete data for required partitionn from disk

As with a queue the consumer group allows you to divide up processing over a collection of processes (the members of the consumer group). As with publish-subscribe, Kafka allows you to broadcast messages to multiple consumer groups. the partitionâ€”within the topics, Kafka is able to provide both ordering guarantees and load balancing over a pool of consumer processes.
Note the key difference between Kafka Streams end-to-end exactly-once guarantee with other stream processing frameworks' claimed guarantees is that Kafka Streams tightly integrates with the underlying Kafka storage system and ensure that commits on the input topic offsets, updates on the state stores, and writes to the output topics will be completed atomically instead of treating Kafka as an external system that may have side-effects.

i guess distributed systems have atleast_once, exactly_once, atmost_once data gaurantees; timing or out of order guarantee problems.

running kafka using command line
http://cloudurable.com/blog/kafka-tutorial-kafka-from-command-line/index.html

kafka doesn't allow more than 1 consumer to read from same partition simultaneously
messages in topic are distributed among partitions
maintaining membership in the group is handled by the Kafka protocol dynamically. 
there is auto distribution of partition ownership among consumers 
zookeeper: required for comm among brokers
Kafka REST Proxy provides a RESTful interface to a Kafka cluster as kafka doesn't provide rest api on its own

kafka arch
https://thehoard.blog/how-kafkas-storage-internals-work-3a29b02e026
http://cloudurable.com/blog/kafka-architecture-low-level/index.html

kafka uses replication for in-sync replicas, which can be made master on failure. its not for read balanacing
we can dynamically change number of partitions, but kafka won't auto redistribute data

kafka optimizations in code
http://cloudurable.com/blog/kafka-tutorial-kafka-producer-advanced-java-examples/index.html

kafka-connect
https://docs.confluent.io/3.0.0/connect/userguide.html
kafka-connect producer and consumer properties:
http://kafka.apache.org/documentation.html#producerconfigs

kafka operations: adding nodes, partitions, reassignments:
https://svn.apache.org/repos/asf/kafka/site/082/ops.html

limiting kafka memory 
https://stackoverflow.com/questions/27681511/how-do-i-set-the-java-options-for-kafka

below fields are stored by kafka into zookeeper
https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper

kafka streams
https://medium.com/@andy.bryant/kafka-streams-work-allocation-4f31c24753cc

kafka compaction internals
https://towardsdatascience.com/log-compacted-topics-in-apache-kafka-b1aa1e4665a7

Exactly once semantics in kafka:
Since 0.11.0.0, the Kafka producer also supports an idempotent delivery option which guarantees that resending will not result in duplicate entries in the log. To achieve this, the broker assigns each producer an ID and deduplicates messages using a sequence number that is sent by the producer along with every message. Also beginning with 0.11.0.0, the producer supports the ability to send messages to multiple topic partitions using transaction-like semantics: i.e. either all messages are successfully written or none of them are. The main use case for this is exactly-once processing between Kafka topics
The consumer's position is stored as a message in a topic, so we can write the offset to Kafka in the same transaction as the output topics receiving the processed data. If the transaction is aborted, the consumer's position will revert to its old value and the produced data on the output topics will not be visible to other consumers, depending on their "isolation level." In the default "read_uncommitted" isolation level, all messages are visible to consumers even if they were part of an aborted transaction, but in "read_committed," the consumer will only return messages from transactions which were committed

Note the key difference between Kafka Streams end-to-end exactly-once guarantee with other stream processing frameworks' claimed guarantees is that Kafka Streams tightly integrates with the underlying Kafka storage system and ensure that commits on the input topic offsets, updates on the state stores, and writes to the output topics will be completed atomically instead of treating Kafka as an external system that may have side-effects.

replication in kafka
https://www.confluent.io/blog/hands-free-kafka-replication-a-lesson-in-operational-simplicity
https://www.cloudkarafka.com/blog/2019-09-28-what-does-in-sync-in-apache-kafka-really-mean.html


__quesitons__:
how number of partitions are decided, can they be cahnged later on
how many consumers should be run in cg
write and verify consumers in cgroup, what stragey is used toi determine number of consumers/cgs 
how is kafka stream scalable, how is transaction maintained in this system

__todo__
check some php lib

https://www.youtube.com/watch?v=CZ3wIuvmHeM




deleting topic directly from fs doesn't delete it













