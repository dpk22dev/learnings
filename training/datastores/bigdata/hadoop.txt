Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2018-09-02T12:00:49+05:30

====== hadoop ======
Created Sunday 02 September 2018

installed in: [[/opt/installs/hadoop-2.9.2]]																																																																																												

ports: https://docs.cloudera.com/documentation/enterprise/5-2-x/topics/cdh_ig_ports_cdh5.html
hdfs 50010, 1004, 50075, 1006, 50020, 50070
50470, 50090, 50495, 8485, 8480, 2049, 4242, 111, 14000, 14001
mr 8021, 50030, 9290, 50060, 
yarn 8032, 8030, 8031, 8033, 8088, 8040, 8042, 8041, 10020, 13562, 19888
hbase 60000, 60020, 60030, 2888, 3888, 8080,9090, 9095, 9090, 11060
hive 9083, 10000
scoop 16000
scoop2 12000, 12001
zk 2181
zkfc 8019
hue 8888
oozie 11000, 11001
spark 7077, 7078, 18080, 18081

checking all ports:
echo "3306,50010, 1004, 50075, 1006, 50020, 50070, 50470, 50090, 50495, 8485, 8480, 2049, 4242, 111, 14000, 14001, 8021, 50030, 9290, 50060, 8032, 8030, 8031, 8033, 8088, 8040, 8042, 8041, 10020, 13562, 19888, 60000, 60020, 60030, 2888, 3888, 8080,9090, 9095, 9090, 11060, 9083, 10000, 16000, 12000, 12001, 2181 ,8019, 8888,11000, 11001, 7077, 7078, 18080, 18081" | sed -n 1'p' | tr ',' '\n' | while read word; do echo "checking: $word"; netstat -ntl | grep $word; done;

https://dzone.com/refcardz/getting-started-apache-hadoop	

hadoop docs
http://hadoop.apache.org/docs/current/

hadoop imp config and booting up simple hadoop system:
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html
$HADOOP_PREFIX/sbin/start-dfs.sh
$HADOOP_PREFIX/sbin/start-yarn.sh
$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver

$HADOOP_PREFIX/sbin/stop-dfs.sh
$HADOOP_PREFIX/sbin/stop-yarn.sh
$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver

after running use:
localhost:50070
localhost:8088
localhost:19888
[[http://localhost:8088/cluster]]

apache hadoop framework ecosystem
https://hortonworks.com/ecosystems/
from ui we can directly put files/data and process with different frameworks
https://hortonworks.com/tutorial/hadoop-tutorial-getting-started-with-hdp/section/1/

tutorial on big data hadoop:
https://www.edureka.co/blog/what-is-big-data/

which technology to use depends on 
Variety of data: structured/semi-structured/unstructured, 
volume of data, 
velocity of data

hadoop ecosystem:
https://www.slideshare.net/uweseiler/introduction-to-the-hadoop-ecosystem-25557364

{{./pasted_image.png}}

{{./pasted_image001.png}}

**Hbase**
http://hbase.apache.org/book.html
It is a sorted map data built on Hadoop. It is column oriented, sparse, distributed, persistent, multidimensional sorted map, which is indexed by rowkey, column key,and timestamp.
Reading a row from HBase requires first checking the MemStore, then the BlockCache, Finally, HFiles on disk are accessed.
https://mapr.com/blog/guidelines-hbase-schema-design/

**Hive:**
Hive is not RDBMS, not used for OLTP. HQL gets reduced to map reduce jobs in backend.

**Pig:**
high level data flow platform for execution Map Reduce programs of Hadoop. The language for Pig is pig Latin.

**sqoop:**
command-line interface application for transferring data between relational databases and Hadoop

**spark:**
Spark is not a modified version of Hadoop and is not, really, dependent on Hadoop because it has its own cluster management. Hadoop is just one of the ways to implement Spark. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application. Most of the Hadoop applications, they spend more than 90% of the time doing HDFS read-write operations. RDD: resilient distributed datasets -> in memory processing computations.
faster iterative, interactive ops on data

spark mlib is fastre than disk based apache mahout

**storm**
nimbus(master), supervisor(worker). Nimbus runs the Storm topology by distributing the task to an available supervisor. A supervisor will have one or more worker process. Supervisor will delegate the tasks to worker processes. Worker process will spawn as many executors as needed and run the task. Apache Storm uses an internal distributed messaging system for the communication between nimbus and supervisors.

**Yarn**
https://www.edureka.co/blog/hadoop-yarn-tutorial/
generic scheduler for distributed apps not just mapreduce apps, replaces job-tracker and task-tracker in hadoop
Resource manager: per cluster
Node manager: per machine
	runs application manager ( per job )
	runs application container ( per task )

bigtable reference paper
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/68a74a85e1662fe02ff3967497f31fda7f32225c.pdf

hadoop 
v1 vs v2 : yarn
v2 vs v3 : mutliple name node servers, dockerization



is hadoop dead
https://siliconangle.com/2018/07/09/hadoops-star-dims-era-cloud-object-data-storage-stream-computing/

**questions**
diff b/w hbase vs hive vs cassandra
using zookeeper for inter machine locking

**todo**
apache hadop + yarn
apache spark
https://www.youtube.com/watch?v=9mELEARcxJo
