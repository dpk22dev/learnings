Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2016-03-09T14:29:45+05:30

====== machine learning and AI ======
Created Wednesday 09 March 2016

games
https://how-to-solve-a-rubix-cube.com/

~~to get terms for linear regression~~
~~https://www.youtube.com/watch?v=GLntx6jKQMY&list=PLr5irXMlOSzjtjyXxrE9aqx9pj-SJ7IWi&index=1~~

machine learning: can follow links to various applications
https://en.wikipedia.org/wiki/Machine_learning

starting with ML
https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/

what is eigen vectors and values
https://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors
https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors

http://machinelearningmastery.com/4-steps-to-get-started-in-machine-learning/

http://www.iamwire.com/2016/10/10-machine-learning-online-courses-for-beginners-2/142589

http://datascience.ibm.com/blog/the-mathematics-of-machine-learning/
https://ocw.mit.edu/courses/mathematics/18-657-mathematics-of-machine-learning-fall-2015/lecture-notes/


http://scikit-learn.org/stable/documentation.html

python ML
https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
https://www.analyticsvidhya.com/blog/2017/05/pydata-amsterdam-2017-machine-learning-deep-learning-data-science/

research @google
https://research.google.com/research-outreach.html#/research-outreach/research-datasets
archive for blogs at
https://research.googleblog.com/2009/06

data mining vs machine learning vs AI
https://stats.stackexchange.com/questions/5026/what-is-the-difference-between-data-mining-statistics-machine-learning-and-ai

**tensor flow **
source: https://github.com/tensorflow/tensorflow/tree/r1.1 at /apps/my-builds/tensorflow
https://www.tensorflow.org/get_started/get_started
docker: https://hub.docker.com/r/tensorflow/tensorflow/

**math **
http://www.souravsengupta.com/cds2016/lectures/Savov_Notes.pdf
http://tutorial.math.lamar.edu/pdf/Calculus_Cheat_Sheet_All.pdf
http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf
http://www.deeplearningbook.org/

**kaggle**
read blogs: winner interview, fork others notebook to understand
https://www.kaggle.com/kaggle/competitions
https://www.kaggle.com/kernels


The systems which exhibit intelligent behavior, learn, demonstrate, explain, and advice its users.
perception, reasoning, problem solving, learning, linguistic intelligence

Usage: Gaming, Natural language processing, Expert systems, speech recognition, handwriting recognition, intelligent robots

 Initially AI problems, including Image Understanding (a really hard one), Signal Processing, Voice Understanding, and Text Understanding, were done by advanced algorithm and coding techniques.
But machine learning is real AI enabling computer with generic understanding. Machine learning create intelligent systems is to have the system itself create and maintain its own World Models. Continuously, in response to sensory input.

eg: in chess checking 10 steps ahead moves is not AI really. system has to learn from inputs what will be most probable step of user this time.

**Intelligence:** The ability of a system to calculate, reason, perceive relationships and analogies, learn from experience, store and retrieve information from memory, solve problems, comprehend complex ideas, use natural language fluently, classify, generalize, and adapt new situations.

intelligence can be:
Linguistic intelligence, Musical intelligence, Logical-mathematical intelligence, Spatial intelligence, Bodily-Kinesthetic intelligence, Intra-personal intelligence, Interpersonal intelligence

**reasoning:** 
1) inductive: from specific observations, make general statement
2) deductive: from general statments, examine the possibility to reach a specific conclusion

**learning:**
auditory 
episodic
motor 
observational
relational
spatial
stimulus response

speech recognition: what was spoken
voice recognition: who has spoken

An **agent** is anything that can perceive its **environment** through sensors and acts upon that environment through **effectors**.

Natural Language Understanding( NLU ) and Natural Language Generation( NLG )
Problems: Lexical Ambiguity, Syntax level ambiguity, referential ambiguity

intelligence is acquiring info, using that info for decision making. but it also includes thinking, creating new thoughts
we can see the intelligence in other animals, but we can't measure it

**Machine learning**
eg: play checker: computer learns the patterns which lead to win and which to loss

A computer is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, imroves with experience E.

**types of learnings**
1) Supervised learning
 we are giving algo input and output data, algo learns the association b/w input and output. predicts the output when nput is given.
**classes**: regression, classification
eg: cancer prediction problems

we can prove 
with how much accuracy can algo give results under what conditions. 
how much training data/iterations are required to keep result within some percentage.

it depends on domain knowledge, how tools of machine learning are used. will they work for this particular problem?

2) Unsupervised learning
in supervised learning we are given training data. input and output is provided. But in unsupervised learning you are not given answer for data.  you have to find some pattern in data.
eg: clustering, segmentation of customers, better quality sound in noise using independent component analysis algo

3) Reinforcement learning
based on rewards and punishments, learning
specify: good and bad behaviours
eg: robotics and web crawlers

gradient descent: sometimes depend on where we started with( or initial theta )
batch gradient descent: requires lot of calucations for large data for calculating theta(i)

stochastic gradient descent: faster for large data set, won't actually converge to actual minimum. parameters will wander around parameters which get us minimum.

linear subspace, basis of subspaces: https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/subspace_basis/v/linear-algebra-basis-of-a-subspace
echelon transformations: http://stattrek.com/matrix-algebra/echelon-transform.aspx
eigen value and vectors: https://www.khanacademy.org/math/linear-algebra/alternate_bases/eigen_everything/v/linear-algebra-eigenvalues-of-a-3x3-matrix

usage of eigen values and vectors:
Eigenvectors make understanding linear transformations easy. They are the "axes" (directions) along which a linear transformation acts simply by "stretching/compressing" and/or "flipping"; eigenvalues give you the factors by which this compression occurs. 
http://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors

wit.ai
intent: action that we want to do
requires 100 expressions per intent; 10 entities per expression

**for matrices and vectors:**
http://algebra.nipissingu.ca/tutorials/matrices.html

**machine learning**
https://www.youtube.com/watch?v=ICKBWIkfeJ8&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH
http://scikit-learn.org/stable/documentation.html

Naive bayes is called naive because it doesn't considers order of words. it considers count of prob of words but not order.

SVM kernel is function which maps from low dimension to higher dimension data.
C parameter: tradeoff b/w smooth decision boundry and classifying training points correctly
	large value of C, more complex decision boundry and correctly classifies training set																																														
G( gamma ): how far effect of single training example reaches; high value of gamma during traning only points near to decision boundry are used, so nearby points influence more and curve the decision boundry
while low value of gamma means far away points are also consdered

octave commands:
eg: y1 = sin( 2*pi*v1 );
 y2 = cos( 2*pi*v1 );
**hold on** % same plot for both graphs
plot( v1, y1 ) % plot v1 as x and y1 as y axis
figure( 1 ) % use different numbers for different plots
subplot( 1,2,1 ) % divide plot in 1x2 grid and access 1st grid
subplot( 1,2,2 ) % access second grid
clf % clear figure
xlabel, ylabel, legend
imagesc(A), colorbar, colormap gray; %same color based on same value in matrix
addpath('path to functions dir') %function should be in same named file, it searches in pwd by default

zeros
ones
eye
magic

***** scikit *****
Ridge regression: the larger the ridge alpha parameter, the higher the bias and the lower the variance.
The C parameter controls the amount of regularization in the LogisticRegression object: a large value for C results in less regularization. 

* modern probability theory******
https://www.youtube.com/watch?v=Tk4ubu7BlSk&list=PL17567A1A3F5DB5E4

elastic net and lasso are bias estimation to find low variances
l1 norm: manhattan distance : | b1 + b2 |
l2 norm: euclidean distance : sqrt( b1^2 + b2^2 )

biased method to estimate fit for regression: we constrain the values of betas using l1 norm, l2 norm



__todo__
**siraj after estimator**
to get terms for linear regression
https://www.youtube.com/watch?v=GLntx6jKQMY&list=PLr5irXMlOSzjtjyXxrE9aqx9pj-SJ7IWi&index=1

matplotlib after basic
http://matplotlib.org/users/tutorials.html

read math below
http://www.souravsengupta.com/cds2016/lectures/Savov_Notes.pdf
http://tutorial.math.lamar.edu/pdf/Calculus_Cheat_Sheet_All.pdf
http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf

numpy, pandas, scipy, matpyplot lib explorations
kaggle
blogs on analytics vidhya

after 
https://www.youtube.com/watch?v=p69khggr1Jo&index=3&list=PL2-dafEMk2A7YdKv4XfKpfbTH5z6rEEj3

http://scikit-learn.org/stable/tutorial/statistical_inference/index.html
http://www.scipy-lectures.org/intro/matplotlib/matplotlib.html
https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python#gs.BqJTBcU
shrinkage and curse of dimensionality google/youtube
what do various parameters suggest in classifier defintino
~~what is ridge, lasso~~
clone sklearn code, go through source code


~~start from:~~
~~https://www.coursera.org/learn/machine-learning/lecture/fKi0M/stochastic-gradient-descent-convergence~~

form model selection
http://scikit-learn.org/stable/tutorial/statistical_inference/index.htm[[http://scikit-learn.org/stable/tutorial/statistical_inference/index.html|l]]

scikit installations
pip install quandl, pandas

*************************************************************************************
**AI**
https://classroom.udacity.com/courses/cs271/lessons/48635748/concepts/486525800923
problem set4
classical planning

__questions__
why summation of theta term is added in logistic regression regularization
can we use neural network classification for regression or prediciton too?
why use mean square root error instead of modulus of error
why we use eigen space, echelon matrix ? what is their meaning

~~google: octave plot crashes on zoom, touch events~~
flowchart, quick start, user guide

