Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2018-09-02T12:00:49+05:30

====== hadoop ======
Created Sunday 02 September 2018

installed in: /opt/installs/hadoop-2.9.2


https://dzone.com/refcardz/getting-started-apache-hadoop	

hadoop docs
http://hadoop.apache.org/docs/current/

hadoop imp config and booting up simple hadoop system:
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html
$HADOOP_PREFIX/sbin/start-dfs.sh
$HADOOP_PREFIX/sbin/start-yarn.sh
$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver

$HADOOP_PREFIX/sbin/stop-dfs.sh
$HADOOP_PREFIX/sbin/stop-yarn.sh
$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver

after running use:
localhost:50070
localhost:8088
localhost:19888
http://localhost:8088/cluster

apache hadoop framework ecosystem
https://hortonworks.com/ecosystems/
from ui we can directly put files/data and process with different frameworks
https://hortonworks.com/tutorial/hadoop-tutorial-getting-started-with-hdp/section/1/

tutorial on big data hadoop:
https://www.edureka.co/blog/what-is-big-data/

which technology to use depends on 
Variety of data: structured/semi-structured/unstructured, 
volume of data, 
velocity of data

hadoop ecosystem:
https://www.slideshare.net/uweseiler/introduction-to-the-hadoop-ecosystem-25557364

{{./pasted_image.png}}

{{./pasted_image001.png}}

**Hbase**
http://hbase.apache.org/book.html
It is a sorted map data built on Hadoop. It is column oriented, sparse, distributed, persistent, multidimensional sorted map, which is indexed by rowkey, column key,and timestamp.
Reading a row from HBase requires first checking the MemStore, then the BlockCache, Finally, HFiles on disk are accessed.
https://mapr.com/blog/guidelines-hbase-schema-design/

**Hive:**
Hive is not RDBMS, not used for OLTP. HQL gets reduced to map reduce jobs in backend.

**Pig:**
high level data flow platform for execution Map Reduce programs of Hadoop. The language for Pig is pig Latin.

**sqoop:**
command-line interface application for transferring data between relational databases and Hadoop

**spark:**
Spark is not a modified version of Hadoop and is not, really, dependent on Hadoop because it has its own cluster management. Hadoop is just one of the ways to implement Spark. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application. Most of the Hadoop applications, they spend more than 90% of the time doing HDFS read-write operations. RDD: resilient distributed datasets -> in memory processing computations.
faster iterative, interactive ops on data

spark mlib is fastre than disk based apache mahout

**storm**
nimbus(master), supervisor(worker). Nimbus runs the Storm topology by distributing the task to an available supervisor. A supervisor will have one or more worker process. Supervisor will delegate the tasks to worker processes. Worker process will spawn as many executors as needed and run the task. Apache Storm uses an internal distributed messaging system for the communication between nimbus and supervisors.

**Yarn**
generic scheduler for distributed apps not just mapreduce apps, replaces job-tracker and task-tracker in hadoop
Resource manager: per cluster
Node manager: per machine
	runs application manager ( per job )
	runs application container ( per task )

bigtable reference paper
https://storage.googleapis.com/pub-tools-public-publication-data/pdf/68a74a85e1662fe02ff3967497f31fda7f32225c.pdf

hadoop 
v1 vs v2 : yarn
v2 vs v3 : mutliple name node servers, dockerization



is hadoop dead
https://siliconangle.com/2018/07/09/hadoops-star-dims-era-cloud-object-data-storage-stream-computing/

**questions**
diff b/w hbase vs hive vs cassandra
using zookeeper for inter machine locking

**todo**
apache hadop + yarn
apache spark
https://www.youtube.com/watch?v=9mELEARcxJo
